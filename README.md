# Fine tune IDEFICS Vision Language Model
This repository demonstrates the data preparation and fine-tuning the Idefics2-8B Vision Language Model.

## Vision Language Model:
Vision Language Models are multimodal models that learn from images and text, generating text outputs from image and text inputs. They excel in zero-shot capabilities, generalization, and various tasks like image recognition, question answering, and document understanding.

<img src="https://github.com/NSTiwari/Fine-tune-IDEFICS-Vision-Language-Model/blob/main/Vision%20Language%20Model.png">

## Dataset

<img src="https://github.com/NSTiwari/Fine-tune-IDEFICS-Vision-Language-Model/blob/main/dataset.png">

## Inference

<img src="https://github.com/NSTiwari/Fine-tune-IDEFICS-Vision-Language-Model/blob/main/test_data.png">

**Question**: What the location address of NSDA? 

**Answer:** ['1128 SIXTEENTH ST., N. W., WASHINGTON, D. C. 20036', '1128 sixteenth st., N. W., washington, D. C. 20036']

## References & Resources:

- Read the [Medium blog]() for step-by-step imeplementation.
- [Vision Language Models](https://huggingface.co/blog/vlms)
- [LoRA & QLoRA](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/lora-qlora)
- [Idefics2-8B Vision Language Model](https://huggingface.co/blog/idefics2)
- [![Open Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/NSTiwari/Fine-tune-IDEFICS-Vision-Language-Model/blob/main/Fine_tune_IDEFICS_Vision_Language_Model.ipynb)

